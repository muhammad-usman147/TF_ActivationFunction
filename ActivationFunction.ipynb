{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What is an ACTIVATION FUNCTION..??\n",
    "ACTIVATION FUNCTION is added with every neural layers.... It determines whether the neuron\n",
    "should be activated of not.. An ACTIVATION FUNCTION also convertes the output from linearity to\n",
    "non-linearity from every neuron. \n",
    "\n",
    "If there is no ACTIVATION FUNCTION, then the neural net will only work on a those data who are separated by\n",
    "linear fashion. \n",
    "\n",
    "I-E :- Prediction the Closing time of a company will not work if there is no ACTIVATION FUNCTION attached to the \n",
    "       neuron layer.Hence, it will behave like a simple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(np.random.randn(2,1),dtype=np.float16)\n",
    "b = tf.Variable([0.0],shape=1,dtype = np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Variable 'Variable:0' shape=(2, 1) dtype=float16, numpy=\n array([[ 0.4172],\n        [-1.74  ]], dtype=float16)>,\n <tf.Variable 'Variable:0' shape=(1,) dtype=float16, numpy=array([0.], dtype=float16)>)"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Defining a Model without an activation funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def forward(X):\n",
    "    output = tf.matmul(X,W)+b\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[ 0.85   -0.3074]]\n"
    }
   ],
   "source": [
    "X = tf.convert_to_tensor(np.random.randn(1,2),dtype=np.float16)\n",
    "print(X.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0.8896]]\n"
    }
   ],
   "source": [
    "print(forward(X).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor([[0.8896]], shape=(1, 1), dtype=float16)\n"
    }
   ],
   "source": [
    "out = forward(X)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- SIGMOID ACTIVATION FUNCTION\n",
    "\n",
    "    RANGE (0,1)\n",
    "    Used in the models where the output is in the binary form predicting whether the input is a \n",
    "    cat or a dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0.7085]]\n"
    }
   ],
   "source": [
    "sigmoid = tf.nn.sigmoid(out)\n",
    "print(sigmoid.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TanH or HyperBolic Tangent Function\n",
    "\n",
    "    Range(-1,1)\n",
    "    Used in the model where the input is in the form of classification\n",
    "    i-e, Cat or Dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh = tf.nn.tanh(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0.7114]]\n"
    }
   ],
   "source": [
    "print(tanh.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELU ( Rectified Linear Unit )\n",
    "    range(0, infinite(+))\n",
    "    One of the most used Activation Function, If you are not sure which Activation Function you \n",
    "    should use, then go for RELU, because it is a non-satured Activation function, which means \n",
    "    it doesnot converge towards a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0.8896]]\n"
    }
   ],
   "source": [
    "relu = tf.nn.relu(out)\n",
    "print(relu.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Softmax Activation Function \n",
    "    range (0,1)\n",
    "    It returns a vector of probabilities depending on the number of classes, the total sum of the \n",
    "    output vector will add upto 1. It is used in the models there is multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[1.]]\n"
    }
   ],
   "source": [
    "softmax = tf.nn.softmax(out)\n",
    "print(softmax.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}